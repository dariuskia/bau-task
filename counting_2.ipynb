{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David Bau stream test task\n",
    "### Darius Kianersi\n",
    "\n",
    "Given the following type of prompt, a sufficiently large language model will be able to answer with the correct number.\n",
    "```\n",
    "Count the number of words in the following list that match the given type, and put the numerical answer in parentheses.\n",
    "Type: fruit\n",
    "List: [dog apple cherry bus cat grape bowl]\n",
    "Answer: (\n",
    "```\n",
    "1. create a dataset of several thousand examples like this.\n",
    "2. benchmark some open-weight LMs on solving this task zero-shot (without reasoning tokens)\n",
    "3. for a single model, create a causal mediation analysis experiment (patching from one run to another) to answer: \"is there a hidden state layer that contains a representation of the running count of matching words, while processing the list of words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "from typing import Literal, Type, TypeAlias\n",
    "import random\n",
    "from __future__ import annotations\n",
    "from typing import TypedDict, List, Literal, Any\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a dataset of these examples\n",
    "Here's my high-level plan:\n",
    "1. Generate a few word types that are mutually exclusive (this makes it easier to get ground truth values).\n",
    "2. Generate some words for each type.\n",
    "3. Sample a balanced dataset of words and a type, then compute the answer for the given type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "MODEL = \"gpt-4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Structured-output schemas\n",
    "class WordTypeList(BaseModel):\n",
    "    categories: List[str] # e.g. [\"animals\", \"fruits\", \"utensils\"]\n",
    "\n",
    "class Word(BaseModel):\n",
    "    name: str # e.g. \"trumpet\"\n",
    "    word_type: str # e.g. \"musical instruments\"\n",
    "\n",
    "class WordList(BaseModel):\n",
    "    words: List[Word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Helpers that wrap the chat.completions.parse call\n",
    "def get_word_types(n: int = 6) -> List[str]:\n",
    "    \"\"\"Return mutually-exclusive word categories.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"Return a JSON object with one field 'categories' \"\n",
    "                    \"containing a list of mutually-exclusive word types.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"Generate {n} mutually exclusive word types like \"\n",
    "                    \"'animals', 'fruits', 'utensils'. \"\n",
    "                    \"Do NOT include overlapping concepts.\"}\n",
    "    ]\n",
    "    resp: WordTypeList = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        response_format=WordTypeList,\n",
    "    )\n",
    "    return json.loads(resp.choices[0].message.content)[\"categories\"]\n",
    "\n",
    "\n",
    "def get_words_for_type(target: str, others: List[str],\n",
    "                       k_min: int = 10, k_max: int = 20) -> List[Word]:\n",
    "    \"\"\"Return Word objects for one category, avoiding overlap with others.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"Return a JSON object with one field 'words': \"\n",
    "                    \"a list of objects each having 'name' and 'word_type'.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"\"\"Generate {k_min}-{k_max} distinct words that are **{target}**.\n",
    "These words must **not** also fit any of the following types:\n",
    "{', '.join(others)}.\n",
    "Return each as an object: \\\"name\\\": <word>, \\\"word_type\\\": \\\"{target}\\\".\n",
    "Exclude plurals if the singular form is already present; no extras.\"\"\",}\n",
    "    ]\n",
    "    resp: WordList = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        response_format=WordList,\n",
    "    )\n",
    "    return json.loads(resp.choices[0].message.content)[\"words\"]\n",
    "\n",
    "\n",
    "# 3. Build the full dataset\n",
    "def build_dataset() -> List[Word]:\n",
    "    categories = get_word_types()\n",
    "    dataset: List[Word] = []\n",
    "    for t in categories:\n",
    "        others = [o for o in categories if o != t]\n",
    "        dataset.extend(get_words_for_type(t, others))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 4. Sample and tally helpers\n",
    "def random_sample(words: List[Word], n: int = 8) -> List[Word]:\n",
    "    return random.sample(words, n)\n",
    "\n",
    "def tally_by_type(words: List[Word]) -> dict[str, int]:\n",
    "    counter = collections.Counter(w[\"word_type\"] for w in words)\n",
    "    return dict(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 118 total words across 6 types.\n",
      "\n",
      " Random sample:\n",
      "      rambutan  —  fruits\n",
      "       Persian  —  languages\n",
      "     ambulance  —  vehicles\n",
      "   rice paddle  —  utensils\n",
      "        zester  —  utensils\n",
      "       apricot  —  fruits\n",
      "         sitar  —  musical instruments\n",
      "    tambourine  —  musical instruments\n",
      "        papaya  —  fruits\n",
      "        cherry  —  fruits\n",
      "          harp  —  musical instruments\n",
      "       panther  —  animals\n",
      "\n",
      " Counts by category:\n",
      "  animals             : 20\n",
      "  fruits              : 20\n",
      "  utensils            : 20\n",
      "  vehicles            : 18\n",
      "  languages           : 20\n",
      "  musical instruments : 20\n"
     ]
    }
   ],
   "source": [
    "all_words = build_dataset()\n",
    "print(f\"Generated {len(all_words)} total words across \"\n",
    "        f\"{len(set(w['word_type'] for w in all_words))} types.\")\n",
    "\n",
    "sample = random_sample(all_words, 12)\n",
    "print(\"\\n Random sample:\")\n",
    "for w in sample:\n",
    "    print(f\"  {w['name']:>12s}  —  {w['word_type']}\")\n",
    "\n",
    "counts = tally_by_type(all_words)\n",
    "print(\"\\n Counts by category:\")\n",
    "for typ, cnt in counts.items():\n",
    "    print(f\"  {typ:<20s}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of samples: 3000\n",
      "\n",
      "Example samples:\n",
      "\n",
      "Type: fruits\n",
      "List: ['grater', 'mango', 'peach', 'apricot', 'papaya', 'scooter', 'skimmer', 'czech']\n",
      "Answer: (4)\n",
      "\n",
      "Type: vehicles\n",
      "List: ['elephant', 'norwegian', 'skimmer', 'rice paddle']\n",
      "Answer: (0)\n",
      "\n",
      "Type: utensils\n",
      "List: ['tram', 'truck', 'grater', 'masher', 'guava']\n",
      "Answer: (2)\n",
      "\n",
      "Type: musical instruments\n",
      "List: ['masher', 'zester', 'violin', 'vietnamese', 'lychee']\n",
      "Answer: (1)\n",
      "\n",
      "Type: musical instruments\n",
      "List: ['cleaver', 'banjo', 'mandolin', 'jeep', 'bicycle', 'marimba']\n",
      "Answer: (3)\n"
     ]
    }
   ],
   "source": [
    "def generate_balanced_samples(all_words, answers_range=(0, 4), samples_per_answer=10):\n",
    "    \"\"\"\n",
    "    Generate balanced samples where:\n",
    "    - Each possible answer (0-4) has the same number of samples\n",
    "    - Each word type has the same number of samples\n",
    "    - For each answer and type combination, generate multiple samples\n",
    "    \"\"\"\n",
    "    # Get all unique types\n",
    "    all_types = list(set(w[\"word_type\"].lower() for w in all_words))\n",
    "    \n",
    "    # Group words by type for easier access\n",
    "    words_by_type = {\n",
    "        t.lower(): [w for w in all_words if w[\"word_type\"].lower() == t.lower()]\n",
    "        for t in all_types\n",
    "    }\n",
    "    \n",
    "    # Store all generated samples\n",
    "    samples = []\n",
    "    \n",
    "    # For each type\n",
    "    for target_type in all_types:\n",
    "        # For each desired answer\n",
    "        for target_answer in range(answers_range[0], answers_range[1] + 1):\n",
    "            # Generate multiple samples for this type-answer combination\n",
    "            for _ in range(samples_per_answer):\n",
    "                # 1. First, decide total list length (should be reasonable given target_answer)\n",
    "                min_len = max(4, target_answer)  # At least 4 words, or enough to fit target_answer\n",
    "                max_len = min(8, target_answer + 4)  # Cap at 8, or target_answer + 4\n",
    "                total_len = random.randint(min_len, max_len)\n",
    "                \n",
    "                # 2. Select target_answer words of the target type\n",
    "                target_words = random.sample(words_by_type[target_type], target_answer)\n",
    "                \n",
    "                # 3. Fill remaining slots with words from other types\n",
    "                remaining_slots = total_len - target_answer\n",
    "                other_words = []\n",
    "                if remaining_slots > 0:\n",
    "                    # Pool of words from other types\n",
    "                    other_types_words = [\n",
    "                        w for w in all_words \n",
    "                        if w[\"word_type\"].lower() != target_type\n",
    "                    ]\n",
    "                    other_words = random.sample(other_types_words, remaining_slots)\n",
    "                \n",
    "                # 4. Combine and shuffle\n",
    "                all_selected_words = target_words + other_words\n",
    "                random.shuffle(all_selected_words)\n",
    "                \n",
    "                # 5. Create the sample\n",
    "                word_list = [w[\"name\"].lower() for w in all_selected_words]\n",
    "                \n",
    "                samples.append({\n",
    "                    \"type\": target_type.lower(),\n",
    "                    \"list\": word_list,\n",
    "                    \"answer\": target_answer\n",
    "                })\n",
    "    \n",
    "    # Shuffle all samples\n",
    "    random.shuffle(samples)\n",
    "    return samples\n",
    "\n",
    "# Test the function\n",
    "samples = generate_balanced_samples(all_words, answers_range=(0, 4), samples_per_answer=100)\n",
    "\n",
    "# Create dataframe and plot distributions\n",
    "df = pd.DataFrame(samples)\n",
    "\n",
    "# Plot distribution by answer\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['answer'].hist(bins=range(df['answer'].min(), df['answer'].max() + 2, 1))\n",
    "plt.title('Distribution of Answers')\n",
    "plt.xlabel('Answer')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution by type\n",
    "plt.figure(figsize=(10, 6))\n",
    "type_counts = df['type'].value_counts()\n",
    "type_counts.plot(kind='bar')\n",
    "plt.title('Distribution by Type')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Frequency') \n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print basic statistics about the samples\n",
    "print(f\"\\nTotal number of samples: {len(df)}\")\n",
    "# Print a few example samples\n",
    "print(\"\\nExample samples:\")\n",
    "for i, sample in df.head().iterrows():\n",
    "    print(f\"\\nType: {sample['type']}\")\n",
    "    print(f\"List: {sample['list']}\")\n",
    "    print(f\"Answer: ({sample['answer']})\")\n",
    "\n",
    "# Save the dataframe to disk\n",
    "df.to_csv('counting_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Eval some open-weight models on this task\n",
    "Let's eval GPT-2 as well as Llama-3.1-8B and Llama-3.1-70B.\n",
    "\n",
    "I'll use the Inspect library from UK AISI for GPT-2; however, afaict it doesn't allow for completions, so I'll reformat the prompt to be conversational/a chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': None,\n",
      " 'files': None,\n",
      " 'id': None,\n",
      " 'input': 'Count the number of words in the following list that match the '\n",
      "          'given type, respond with the numerical answer only.\\n'\n",
      "          'Type: fruits\\n'\n",
      "          'List: grater mango peach apricot papaya scooter skimmer czech\\n'\n",
      "          'Answer: ',\n",
      " 'metadata': None,\n",
      " 'sandbox': None,\n",
      " 'setup': None,\n",
      " 'target': '4'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from inspect_ai.dataset import Sample, csv_dataset\n",
    "from inspect_ai.model import ChatMessageAssistant, ChatMessageSystem, ChatMessageUser\n",
    "\n",
    "CHAT_QUESTION_TEMPLATE = \"\"\"Count the number of words in the following list that match the given type, respond with the numerical answer only.\n",
    "Type: {type}\n",
    "List: {list}\n",
    "Answer: \"\"\"\n",
    "\n",
    "def format_sample_chat(record: dict[str, Any]) -> Sample:\n",
    "    \"\"\"\n",
    "    Formats counting dataset into inspect_ai Sample objects.\n",
    "    \"\"\"\n",
    "    input = CHAT_QUESTION_TEMPLATE.format(type=record[\"type\"], list=\" \".join(eval(record[\"list\"])))\n",
    "    target = str(record[\"answer\"])\n",
    "\n",
    "    # return sample\n",
    "    return Sample(input=input, target=target)\n",
    "\n",
    "dataset = csv_dataset(\"counting_samples.csv\", name=\"Counting\", sample_fields=format_sample_chat)\n",
    "pprint(dataset.samples[0].__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d129f029be485b987ff0ebd9f7cea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from inspect_ai import Task, task\n",
    "from inspect_ai import eval as inspect_eval\n",
    "from inspect_ai.scorer import exact\n",
    "from inspect_ai.solver import generate\n",
    "\n",
    "@task\n",
    "def counting_task() -> Task:\n",
    "    return Task(\n",
    "        dataset=dataset,\n",
    "        solver=[generate()],\n",
    "        scorer=exact(),\n",
    "    )\n",
    "\n",
    "# model_name = \"openai/gpt-4o-mini\"\n",
    "model_name = \"hf/openai-community/gpt2\"\n",
    "# model = get_model(model_name, device=\"cuda:0\")\n",
    "results = inspect_eval(counting_task(), model=model_name, max_tokens=1, limit=3000, log_format=\"eval\", log_dir=\"logs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is ~0.16%, which is 5 samples out of 3000 counted correctly. GPT-2 is quite small (120M), so we don't expect much better. \n",
    "\n",
    "Let's try Llama-3.1-8B. For this, I'll create my own eval harness using the original completion prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "COMPLETION_QUESTION_TEMPLATE = \"\"\"Count the number of words in the following list that match the given type, and put the numerical answer in parentheses.\n",
    "Type: {type}\n",
    "List: {list}\n",
    "Answer: (\"\"\"\n",
    "device = 'cuda'\n",
    "\n",
    "class CountingDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        formatted_prompt = COMPLETION_QUESTION_TEMPLATE.format(type=row[\"type\"], list=\" \".join(eval(row[\"list\"])))\n",
    "        return formatted_prompt, str(row['answer'])\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    formatted_prompts, answers = zip(*batch)\n",
    "    x = tokenizer(formatted_prompts, return_tensors=\"pt\", padding=True)\n",
    "    y = tokenizer(answers, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    return x, y\n",
    "\n",
    "dataset = CountingDataset(\"counting_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def eval_model(model, dataset, tokenizer):\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=partial(collate_fn, tokenizer=tokenizer))\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataset)//dataloader.batch_size)\n",
    "    for x, y in pbar:\n",
    "        with torch.inference_mode():\n",
    "            out = model(input_ids=x['input_ids'].to(device), attention_mask=x['attention_mask'].to(device))\n",
    "            logits = out.logits\n",
    "            probs = logits[:, -1].softmax(dim=-1)\n",
    "            next_toks = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            total_correct += (next_toks == y['input_ids'].to(device)).sum()\n",
    "            total_seen += len(x['input_ids'])\n",
    "            pbar.set_description(f\"Acc: {total_correct/total_seen:.2%}\")\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f899bf85cad41739481f29401fb65c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc: 44.77%: : 94it [00:05, 16.43it/s]                      \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "llama_8b = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=partial(collate_fn, tokenizer=tokenizer))\n",
    "\n",
    "eval_model(llama_8b, dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "44% is much better, especially since this isn't a multiple choice eval — the model sees between 4 and 8 words in each list, so this is better than random guessing based off the list length.\n",
    "\n",
    "However, we'd probably still like a greater accuracy to be confident the circuit exists and the model doesn't just have decent guesswork heuristics. Let's try 70B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d96cca8fa784481b0dfd514de308d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-70B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "llama_70b = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc: 64.87%: : 94it [00:40,  2.34it/s]                      \n"
     ]
    }
   ],
   "source": [
    "eval_model(llama_70b, dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Causal Mediation Analysis\n",
    "I have two ideas for what the circuit might look like:\n",
    "1. Model is keeping a running count of fruits\n",
    "    - Attn is moving the current count from a prev position to current position, updating it (+1) with the MLP\n",
    "    - Final answer attends to last fruit, gets its count\n",
    "2. Model only stores whether each position is a fruit, adds them together at the end\n",
    "    - Attention on a given fruit doesn't move info from previous fruits\n",
    "    - Each position keeps track of whether it's a fruit (0 or 1)\n",
    "    - Final answer attends to all previous fruits, adds them all together in the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I unfortunately was not able to complete this section. I ran into several issues trying to activation patch Llama-3.1-70B that I didn't have time to debug.\n",
    "1. I first tried the `nnsight` library with a remote NDIF instance of the model. This worked for the clean and corrupted forward passes; however, when I tried to batch over all of the layers and token positions with `tracer.invoke`, the job would mysteriously get killed without any error message. I suspected this was an OOM, so I refactored the code to iteratively call `tracer.trace` without batching, but this was unusably slow (1 iter/~30m)\n",
    "2. I used on-demand compute to locally serve the model; however, this OOMed in the batched pass, even when iterating over layers and only batching on the token positions.\n",
    "3. I then tried the `transformer_lens` library, but had no luck here either. The model was not sharding across my GPUs properly and failed with device asserts.\n",
    "\n",
    "If given more time, I would have tried first attribution to localize the relevant layers and token positions with only 2 forward passes and 1 backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import patching\n",
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.components import MLP, Embed, LayerNorm, Unembed\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from plotly.express import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4186ba2a9da04322a64f60b0fb1fb751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-70B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-70B\",\n",
    "    dtype=torch.float16,\n",
    "    device='cuda',\n",
    "    n_devices=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "device_type = next(iter(model.parameters())).device.type\n",
    "clean_prompt = \"\"\"Count the number of words in the following list that match the given type, and put the numerical answer in parentheses.\n",
    "Type: fruit\n",
    "List: [dog apple cherry bus cat grape bowl]\n",
    "Answer: (\"\"\"\n",
    "corrupted_prompt = \"\"\"Count the number of words in the following list that match the given type, and put the numerical answer in parentheses.\n",
    "Type: fruit\n",
    "List: [dog apple spoon bus cat grape bowl]\n",
    "Answer: (\"\"\"\n",
    "prompts = (clean_prompt, corrupted_prompt)\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompt).to(device_type)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt).to(device_type)\n",
    "\n",
    "answers = (\"3\", \"2\")\n",
    "answer_tokens = model.to_tokens(answers, prepend_bos=False).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:3! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m clean_logits, clean_cache = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlogits_to_ave_logit_diff\u001b[39m(logits, answer_tokens):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:694\u001b[39m, in \u001b[36mHookedTransformer.run_with_cache\u001b[39m\u001b[34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_with_cache\u001b[39m(\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m, *model_args, return_cache_object=\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs\n\u001b[32m    679\u001b[39m ) -> Tuple[\n\u001b[32m   (...)\u001b[39m\u001b[32m    686\u001b[39m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]],\n\u001b[32m    687\u001b[39m ]:\n\u001b[32m    688\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[32m    689\u001b[39m \n\u001b[32m    690\u001b[39m \u001b[33;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[32m    691\u001b[39m \u001b[33;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[33;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m     out, cache_dict = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[32m    698\u001b[39m         cache = ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim=\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/hook_points.py:569\u001b[39m, in \u001b[36mHookedRootModule.run_with_cache\u001b[39m\u001b[34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m cache_dict, fwd, bwd = \u001b[38;5;28mself\u001b[39m.get_caching_hooks(\n\u001b[32m    556\u001b[39m     names_filter,\n\u001b[32m    557\u001b[39m     incl_bwd,\n\u001b[32m   (...)\u001b[39m\u001b[32m    560\u001b[39m     pos_slice=pos_slice,\n\u001b[32m    561\u001b[39m )\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(\n\u001b[32m    564\u001b[39m     fwd_hooks=fwd,\n\u001b[32m    565\u001b[39m     bwd_hooks=bwd,\n\u001b[32m    566\u001b[39m     reset_hooks_end=reset_hooks_end,\n\u001b[32m    567\u001b[39m     clear_contexts=clear_contexts,\n\u001b[32m    568\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m     model_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[32m    571\u001b[39m         model_out.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:630\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    628\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munembed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_vocab]\u001b[39;00m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.output_logits_soft_cap > \u001b[32m0.0\u001b[39m:\n\u001b[32m    632\u001b[39m         logits = \u001b[38;5;28mself\u001b[39m.cfg.output_logits_soft_cap * F.tanh(\n\u001b[32m    633\u001b[39m             logits / \u001b[38;5;28mself\u001b[39m.cfg.output_logits_soft_cap\n\u001b[32m    634\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/components/unembed.py:31\u001b[39m, in \u001b[36mUnembed.forward\u001b[39m\u001b[34m(self, residual)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mself\u001b[39m, residual: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mbatch pos d_model\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     30\u001b[39m ) -> Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mbatch pos d_vocab_out\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbatch_addmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mb_U\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW_U\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/utilities/addmm.py:33\u001b[39m, in \u001b[36mbatch_addmm\u001b[39m\u001b[34m(bias, weight, x)\u001b[39m\n\u001b[32m     31\u001b[39m n_output_features = weight.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m     32\u001b[39m size_out = x.size()[:-\u001b[32m1\u001b[39m] + (n_output_features,)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m x = \u001b[43mvanilla_addmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m x = x.view(size_out)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/utilities/addmm.py:18\u001b[39m, in \u001b[36mvanilla_addmm\u001b[39m\u001b[34m(input, mat1, mat2)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvanilla_addmm\u001b[39m(\n\u001b[32m     10\u001b[39m     \u001b[38;5;28minput\u001b[39m: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33m... #o\u001b[39m\u001b[33m\"\u001b[39m],  \u001b[38;5;66;03m# Must be broadcastable to \"m o\"\u001b[39;00m\n\u001b[32m     11\u001b[39m     mat1: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mm n\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     12\u001b[39m     mat2: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mn o\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     13\u001b[39m ) -> Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mm o\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     14\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Typechecked version of torch.addmm.\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[33;03m    Note that both mat1 and mat2 *must* be 2d matrices.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:3! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "def logits_to_ave_logit_diff(logits, answer_tokens):\n",
    "    correct_token, incorrect_token = answer_tokens\n",
    "    return (logits[0, -1, correct_token[0]] - logits[0, -1, incorrect_token[0]]) / 2\n",
    "\n",
    "clean_logit_diff = logits_to_ave_logit_diff(clean_logits, answer_tokens)\n",
    "corrupted_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n",
    "\n",
    "def ioi_metric(logits, answer_tokens=answer_tokens, corrupted_logit_diff=corrupted_logit_diff, clean_logit_diff=clean_logit_diff):\n",
    "    patched_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logit_diff = torch.tensor(1.28125)\n",
    "corrupted_logit_diff = torch.tensor(-1.69921875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.28125, -1.69921875)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logit_diff.item(), corrupted_logit_diff.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddedfb79e2a4bf9a8a0d6cd8b454724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 139.83 GiB of which 11.56 MiB is free. Including non-PyTorch memory, this process has 139.80 GiB memory in use. Of the allocated memory 138.93 GiB is allocated by PyTorch, and 216.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m act_patch_resid_pre = \u001b[43mpatching\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_act_patch_resid_pre\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrupted_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorrupted_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatching_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioi_metric\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m labels = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtok\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, tok \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model.to_str_tokens(clean_tokens[\u001b[32m0\u001b[39m]))]\n\u001b[32m      7\u001b[39m imshow(\n\u001b[32m      8\u001b[39m     act_patch_resid_pre,\n\u001b[32m      9\u001b[39m     labels={\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPosition\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLayer\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     width=\u001b[32m700\u001b[39m,\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/patching.py:218\u001b[39m, in \u001b[36mgeneric_activation_patch\u001b[39m\u001b[34m(model, corrupted_tokens, clean_cache, patching_metric, patch_setter, activation_name, index_axis_names, index_df, return_index_df)\u001b[39m\n\u001b[32m    211\u001b[39m current_hook = partial(\n\u001b[32m    212\u001b[39m     patching_hook,\n\u001b[32m    213\u001b[39m     index=index,\n\u001b[32m    214\u001b[39m     clean_activation=clean_cache[current_activation_name],\n\u001b[32m    215\u001b[39m )\n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# Run the model with the patching hook and get the logits!\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m patched_logits = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcorrupted_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_activation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_hook\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Calculate the patching metric and store\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flattened_output:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/hook_points.py:456\u001b[39m, in \u001b[36mHookedRootModule.run_with_hooks\u001b[39m\u001b[34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     logging.warning(\n\u001b[32m    452\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    453\u001b[39m     )\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhooked_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:612\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    608\u001b[39m         shortformer_pos_embed = shortformer_pos_embed.to(\n\u001b[32m    609\u001b[39m             devices.get_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m.cfg)\n\u001b[32m    610\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     residual = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/components/transformer_block.py:186\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[39m\n\u001b[32m    182\u001b[39m     mlp_in = (\n\u001b[32m    183\u001b[39m         resid_mid \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_hook_mlp_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hook_mlp_in(resid_mid.clone())\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    185\u001b[39m     normalized_resid_mid = \u001b[38;5;28mself\u001b[39m.ln2(mlp_in)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     mlp_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_resid_mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     resid_post = \u001b[38;5;28mself\u001b[39m.hook_resid_post(resid_mid + mlp_out)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.parallel_attn_mlp:\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.\u001b[39;00m\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/components/transformer_block.py:210\u001b[39m, in \u001b[36mTransformerBlock.apply_mlp\u001b[39m\u001b[34m(self, normalized_resid)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_mlp\u001b[39m(\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m, normalized_resid: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mbatch pos d_model\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    204\u001b[39m ) -> Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mbatch pos d_model\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    205\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Centralized point where the MLP is applied to the forward pass\u001b[39;00m\n\u001b[32m    206\u001b[39m \n\u001b[32m    207\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m        Float[torch.Tensor, \"batch pos d_model\"]: Our resulting tensor\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     mlp_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_resid\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_normalization_before_and_after:\n\u001b[32m    212\u001b[39m         mlp_out = \u001b[38;5;28mself\u001b[39m.ln2_post(mlp_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformer_lens/components/mlps/gated_mlp.py:68\u001b[39m, in \u001b[36mGatedMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     65\u001b[39m     post_act = \u001b[38;5;28mself\u001b[39m.hook_post(\u001b[38;5;28mself\u001b[39m.ln(mid_act))\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     67\u001b[39m     pre_linear = \u001b[38;5;28mself\u001b[39m.hook_pre_linear(\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW_in\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# batch pos d_model, d_model d_mlp -> batch pos d_mlp\u001b[39;00m\n\u001b[32m     69\u001b[39m     )\n\u001b[32m     71\u001b[39m     post_act = \u001b[38;5;28mself\u001b[39m.hook_post(\n\u001b[32m     72\u001b[39m         (\u001b[38;5;28mself\u001b[39m.act_fn(pre_act) * pre_linear) + \u001b[38;5;28mself\u001b[39m.b_in\n\u001b[32m     73\u001b[39m     )  \u001b[38;5;66;03m# [batch, pos, d_mlp]\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch_addmm(\u001b[38;5;28mself\u001b[39m.b_out, \u001b[38;5;28mself\u001b[39m.W_out, post_act)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 139.83 GiB of which 11.56 MiB is free. Including non-PyTorch memory, this process has 139.80 GiB memory in use. Of the allocated memory 138.93 GiB is allocated by PyTorch, and 216.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "act_patch_resid_pre = patching.get_act_patch_resid_pre(\n",
    "    model=model, corrupted_tokens=corrupted_tokens, clean_cache=clean_cache, patching_metric=ioi_metric\n",
    ")\n",
    "\n",
    "labels = [f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))]\n",
    "\n",
    "imshow(\n",
    "    act_patch_resid_pre,\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    "    x=labels,\n",
    "    title=\"resid_pre Activation Patching\",\n",
    "    width=700,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import NNsight, LanguageModel, CONFIG\n",
    "CONFIG.set_default_api_key(os.getenv(\"NDIF_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_prompt = \"\"\"Count the number of words in the following list that match the given type, and put the numerical answer in parentheses.\n",
    "Type: fruit\n",
    "List: [dog apple cherry bus cat grape bowl]\n",
    "Answer: (\"\"\"\n",
    "corrupted_prompt = \"\"\"Count the number of words in the following list that match the given type, and put the numerical answer in parentheses.\n",
    "Type: fruit\n",
    "List: [dog apple spoon bus cat grape bowl]\n",
    "Answer: (\"\"\"\n",
    "prompts = (clean_prompt, corrupted_prompt)\n",
    "\n",
    "answers = (\"3\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6864fccb7c1439d81c8284070632ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f5aca19833433fa745695bbebfc450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d493e4689aae40e9a9036fd589653c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cb1fb68a544833b2f622cb7085e558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LanguageModel(\"meta-llama/Meta-Llama-3.1-70B\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=tensor([[128000,   2568,    279,   1396,    315,   4339,    304,    279,   2768,\n",
      "           1160,    430,   2489,    279,   2728,    955,     11,    323,   2231,\n",
      "            279,  35876,   4320,    304,  75075,    627,    941,     25,  14098,\n",
      "            198,    861,     25,    510,  18964,  24149,  41980,   5951,   8415,\n",
      "          52252,  19763,    933,  16533,     25,    320],\n",
      "        [128000,   2568,    279,   1396,    315,   4339,    304,    279,   2768,\n",
      "           1160,    430,   2489,    279,   2728,    955,     11,    323,   2231,\n",
      "            279,  35876,   4320,    304,  75075,    627,    941,     25,  14098,\n",
      "            198,    861,     25,    510,  18964,  24149,  46605,   5951,   8415,\n",
      "          52252,  19763,    933,  16533,     25,    320]])\n"
     ]
    }
   ],
   "source": [
    "tokens = model.tokenizer(prompts, return_tensors=\"pt\")['input_ids']\n",
    "print(f\"{tokens=}\")\n",
    "correct_idx, incorrect_idx = [model.tokenizer(answer, add_special_tokens=False)['input_ids'][0] for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e631fdb4decb4fe2a4e0291b96cc7018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/59.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28c786309cb45f59908eb4894aeda4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601f06327fa64ad49647a314a3c8792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9b3cda4c614673ab6a56efdbc7a62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04121a6205114bafb5409fb40d10f514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66be12276655432eae29c575c9ae9fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa10e5920c10437688828d7d86dd66aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00030.safetensors:   0%|          | 0.00/4.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a69384ea7de4bd98a1e2604626a5adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b46dbacff64e27a4f3068df736ce79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33d55934114411ba159170f39cad240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015b13ad6ba747cf84915c0af727b330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d11d4e9682f405b8ebe5729b20c5b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec3d5f359e849548dcf26981e5cf671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900958a7406e46bcbf275f1414951732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a64d88567d45468c751302cace0d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0bd51229e34947859e6d6b7bd50e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc3fa018c0f46318ff4f691c244c46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee015309e6f4f24b6480940a1ba8b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99d803f50e04f2ca3e1359600a544e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef4b6ffc14345c1a4bb934a52d56652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b567c341d4884f1885264638688115a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e6dcf5379d4d76a51e08dc95cf5a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a17474c9694ac89be3aa1f56126a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827a01f51aa74a46b996b34101a72832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90a1ca2731f4b728380298eb75a6d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d7a6013f95495e932b9eca527242b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b3a769e302460c97c402873e72cfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfd252b819f4a56a392722d47a976cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b934b59692c04b7c938ac2a862373263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0765852c7d454e52a0d1380d809b3afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc155b5ecb147298f3eb8b0002ad502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a871e83273449d8890e555bffbcd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00030.safetensors:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00d32c5f87c4cc5b3afd04c4d6d8889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e88d0ea0ae241c486fb11280e195e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_LAYERS = len(model.model.layers)\n",
    "with model.trace(clean_prompt) as tracer:\n",
    "    clean_hs = [model.model.layers[layer_idx].output[0].save() for layer_idx in range(N_LAYERS)]\n",
    "    clean_logits = model.lm_head.output\n",
    "\n",
    "        # Calculate the difference between the correct answer and incorrect answer for the clean run and save it.\n",
    "    clean_logit_diff = (clean_logits[0, -1, correct_idx] - clean_logits[0, -1, incorrect_idx]).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace(corrupted_prompt) as tracer:\n",
    "    corrupted_logits = model.lm_head.output\n",
    "\n",
    "        # Calculate the difference between the correct answer and incorrect answer for the corrupted run.\n",
    "    corrupted_logit_diff = (corrupted_logits[0, -1, correct_idx] - corrupted_logits[0, -1, incorrect_idx]).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:09<00:00,  8.22it/s]\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "NNsightError",
     "evalue": "CUDA out of memory. Tried to allocate 15.07 GiB. GPU 0 has a total capacity of 139.81 GiB of which 12.57 GiB is free. Process 349837 has 127.23 GiB memory in use. Of the allocated memory 124.99 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"/venv/main/lib/python3.12/site-packages/nnsight/tracing/graph/node.py\", line 289, in execute",
      "    self.target.execute(self)",
      "  File \"/venv/main/lib/python3.12/site-packages/nnsight/intervention/contexts/interleaving.py\", line 161, in execute",
      "    graph.model.interleave(interleaver, *invoker_args, fn=method,**kwargs, **invoker_kwargs)",
      "  File \"/venv/main/lib/python3.12/site-packages/nnsight/modeling/mixins/meta.py\", line 52, in interleave",
      "    return super().interleave(*args, **kwargs)",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/nnsight/intervention/base.py\", line 341, in interleave",
      "    with interleaver:",
      "  File \"/venv/main/lib/python3.12/site-packages/nnsight/intervention/interleaver.py\", line 129, in __exit__",
      "    raise exc_val",
      "  File \"/venv/main/lib/python3.12/site-packages/nnsight/intervention/base.py\", line 342, in interleave",
      "    return fn(*args, **kwargs)",
      "           ^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/nnsight/modeling/language.py\", line 297, in _execute",
      "    return self._model(",
      "           ^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl",
      "    return self._call_impl(*args, **kwargs)",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl",
      "    return inner()",
      "           ^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1805, in inner",
      "    result = forward_call(*args, **kwargs)",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/accelerate/hooks.py\", line 175, in new_forward",
      "    output = module._old_forward(*args, **kwargs)",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/transformers/utils/generic.py\", line 969, in wrapper",
      "    output = func(self, *args, **kwargs)",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward",
      "    outputs: BaseModelOutputWithPast = self.model(",
      "                                       ^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl",
      "    return self._call_impl(*args, **kwargs)",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl",
      "    return inner()",
      "           ^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1805, in inner",
      "    result = forward_call(*args, **kwargs)",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/transformers/utils/generic.py\", line 969, in wrapper",
      "    output = func(self, *args, **kwargs)",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward",
      "    layer_outputs = decoder_layer(",
      "                    ^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/transformers/modeling_layers.py\", line 48, in __call__",
      "    return super().__call__(*args, **kwargs)",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl",
      "    return self._call_impl(*args, **kwargs)",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl",
      "    return inner()",
      "           ^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1805, in inner",
      "    result = forward_call(*args, **kwargs)",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/accelerate/hooks.py\", line 175, in new_forward",
      "    output = module._old_forward(*args, **kwargs)",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py\", line 324, in forward",
      "    hidden_states = self.mlp(hidden_states)",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl",
      "    return self._call_impl(*args, **kwargs)",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl",
      "    return inner()",
      "           ^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1805, in inner",
      "    result = forward_call(*args, **kwargs)",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/accelerate/hooks.py\", line 175, in new_forward",
      "    output = module._old_forward(*args, **kwargs)",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py\", line 162, in forward",
      "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl",
      "    return self._call_impl(*args, **kwargs)",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl",
      "    return inner()",
      "           ^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1805, in inner",
      "    result = forward_call(*args, **kwargs)",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/modules/activation.py\", line 432, in forward",
      "    return F.silu(input, inplace=self.inplace)",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/nn/functional.py\", line 2380, in silu",
      "    return torch._C._nn.silu(input)",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.07 GiB. GPU 0 has a total capacity of 139.81 GiB of which 12.57 GiB is free. Process 349837 has 127.23 GiB memory in use. Of the allocated memory 124.99 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "",
      "During handling of the above exception, another exception occurred:",
      "",
      "Traceback (most recent call last):",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main",
      "    ",
      "  File \"<frozen runpy>\", line 88, in _run_code",
      "    ",
      "",
      "NNsightError: CUDA out of memory. Tried to allocate 15.07 GiB. GPU 0 has a total capacity of 139.81 GiB of which 12.57 GiB is free. Process 349837 has 127.23 GiB memory in use. Of the allocated memory 124.99 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "ioi_patching_results = []\n",
    "with model.trace() as tracer:\n",
    "    for layer_idx in tqdm(range(N_LAYERS)):\n",
    "        _ioi_patching_results = []\n",
    "\n",
    "        # Iterate through all tokens\n",
    "        for token_idx in range(len(tokens[-1])):\n",
    "            # Patching corrupted run at given layer and token\n",
    "            with tracer.invoke(corrupted_prompt) as invoker:\n",
    "                # Apply the patch from the clean hidden states to the corrupted hidden states.\n",
    "                model.model.layers[layer_idx].output[0][:, token_idx, :] = clean_hs[layer_idx][:, token_idx, :]\n",
    "\n",
    "                patched_logits = model.lm_head.output\n",
    "\n",
    "                patched_logit_diff = (\n",
    "                    patched_logits[0, -1, correct_idx]\n",
    "                    - patched_logits[0, -1, incorrect_idx]\n",
    "                )\n",
    "\n",
    "                # Calculate the improvement in the correct token after patching.\n",
    "                patched_result = (patched_logit_diff - corrupted_logit_diff) / (\n",
    "                    clean_logit_diff - corrupted_logit_diff\n",
    "                )\n",
    "\n",
    "                _ioi_patching_results.append(patched_result.item().save())\n",
    "\n",
    "        ioi_patching_results.append(_ioi_patching_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
